import h5py
import json
import numpy as np
import pandas as pd

from bisect import bisect_left, bisect_right
from math import floor
from random import randint

# from memory_profiler import profile
from icecream import ic

from data_link import *
from dev_helper import record_nm


class EcgRecord:
    """Handles reading a .h5 file of ECG signal data, and sending datasets to graph plot.

    The .h5 file is created by `Ablation Database (ABLDB)`.
    Each .h5 file, contains a collection of .log files for a single surgery.
    Each .log file is a ECG signal dataset generated by sensor by Abbott,
    and it's one Segment of signal taken, artificially sliced.

    All .log files joined sequentially will form the entire sampling of all ECG data in the surgery.
    """

    EPOCH_START = pd.Timestamp('1970-01-01')
    TIME_STRT = str(EPOCH_START)
    UNIT_1US = pd.Timedelta(1, unit='us')
    FMT_TMLB = '%H:%M:%S.%f'

    # @profile
    def __init__(self, path):
        self.record = h5py.File(path, 'r')
        self._seg_keys = list(self.record.keys())  # keys to each segment compiled in the .h5 file
        self.annotatns, self._ann_tm = self._get_annotations()
        self._num_ann = len(self._ann_tm)

        # Following properties are the same across different segments, as far as EcgApp is concerned.
        metadata = self._get_seg(self._seg_keys[0]).get_metadata()
        self.spl_rate = metadata['sample_rate']
        # Multiplying factor for converting to time in microseconds
        self.FAC_TO_US = 10 ** 6 / self.spl_rate
        self.lead_nms = [lead['name'] for lead in metadata['sigheader']]
        self.is_negative = [lead['isNegative'] for lead in metadata['sigheader']]

        self._sample_counts = self._get_sample_counts()
        self._sample_counts_acc = self._get_sample_counts_acc()  # Accumulated
        self.COUNT_END = self._sample_counts_acc[-1] - 1  # Inclusive
        self.TIME_END = str(self.count_to_pd_time(self.COUNT_END))

    def _get_annotations(self):
        annotations = json.loads(self.record.attrs['annotations'])
        anns = []
        anns_tm = []  # Just the time, for finding annotations in time range
        strt_ms = annotations[0]['time_ms']
        for ann in annotations[2:]:  # Skip the first 2 rows
            text = ann['data']['text'] if 'text' in ann['data'] else ''
            # A List which is compatible to JavaScript clientside function
            t = ann['time_ms'] - strt_ms
            anns.append([ann['type'], t, text])
            anns_tm.append(t)
        return anns, anns_tm

    def _get_sample_counts(self):
        """ Helps to check which segment(s) is a time range located in """
        sample_counts = []
        for key in self._seg_keys:
            sample_counts.append(self.record[key].shape[1])
        return sample_counts

    def _get_sample_counts_acc(self):
        lst = [self._sample_counts[0]]
        for i, v in enumerate(self._sample_counts[1:]):
            val = v + lst[i]
            lst.append(val)
        return lst

    def _get_seg(self, key):
        """
        :param key: A key to a segment in the internal dictionary
        :return: A single segment
        """
        return self.Segment(self.record[key])

    def _get_dset_by_idx(self, idx_seg):
        """
        Syntactic sugar for getting dataset of a segment

        .. note:: Can't further read in a channel by index *while adopting* array slicing
        """
        return self.record[self._seg_keys[idx_seg]]

    def get_annotation_header(self):
        return self.annotatns[0]

    def get_annotations(self):
        """
        :return: All annotations across all segments
        """
        return self.annotatns[2:]  # The first 2 elements are header and protocol respectively

    class Segment:
        """
        Each segment contains multiple `leads` taking in data concurrently
        """

        def __init__(self, dataset):
            """:param: dataset: A dataset in a .h5 file, 2 dimensional, #leads * #sample """
            self.dataset = dataset

        def get_metadata(self):
            return json.loads(self.dataset.attrs['metadata'])

    def _locate_seg_idx(self, strt, end):
        """ Locates the segment(s) the sample_range spans, by index

        :param strt: integer start sample-count
        :param end: integer end sample-count
        :return: start, end segment-index tuple

        .. note:: In desencding frequency, sample_range should lie in 1 segment, or 2 segments if at edge, \\
        or multiple segments on sample global view
        """
        # Edge case when idx_end = edge of segment start, still need to include that 1 value hence `bisect_right`
        return bisect_left(self._sample_counts_acc, strt), bisect_right(self._sample_counts_acc, end)

    # @profile
    def get_ecg_samples(self, idx_lead, strt, end, step=1):
        """ Continuous samples of ecg magnitudes, specified by counted range

        :param idx_lead: index of the lead
        :param strt: start sample count
        :param end: end sample count
        :param step: every `step`-th value in the data sample is included
        :return: 1D array of ecg values

        .. note:: optimized for large sample_range
        .. seealso:: `EcgApp._Plot.get_fig()`
        """
        idx_strt, idx_end = self._locate_seg_idx(strt, end)
        if idx_strt != 0:
            strt = strt - self._sample_counts_acc[idx_strt - 1]
        if idx_end != 0:
            end = end - self._sample_counts_acc[idx_end - 1]
        if end < self.COUNT_END:
            end += 1  # for inclusive end

        if idx_strt == idx_end:
            return self._get_dset_by_idx(idx_strt)[idx_lead, strt:end:step]
        else:
            dset = self._get_dset_by_idx(idx_strt)
            parts = [dset[idx_lead, strt::step]]
            # e.g. Shape is 20 so indices [0, 19], strt is 2 and step is 7
            # => returns values at indices 2, 9, 16 with 3 elements remaining
            offset_prev = self._get_prev_remaining_offset(dset.shape[1], strt, step)
            for i in range(idx_strt + 1, idx_end):  # for range()'s exclusive end
                dset = self._get_dset_by_idx(i)
                # The new start index relative to this segment, note 0-indexing
                strt = step - 1 - offset_prev  # Sanity check: the new `strt` is in the range [0, step)
                parts.append(dset[idx_lead, strt::step])
                offset_prev = self._get_prev_remaining_offset(dset.shape[1], strt, step)
            strt = step - 1 - offset_prev
            parts.append(self._get_dset_by_idx(idx_end)[idx_lead, strt:end:step])
            return np.concatenate(parts)

    @staticmethod
    def _get_prev_remaining_offset(sz_arr, strt, step):
        """
        So that stepping across multiple array segments behaves as if a single array

        Achieved by carrying over `step` offsets

        :return: Integer in range [0, step)
        """
        return (sz_arr - 1 - strt) % step

    def get_global_samples(self, idx_lead, step):
        """ For plot global thumbnail, data taken at large samples """
        parts = []
        for k in self._seg_keys:
            parts.append(self.record[k][idx_lead, ::step])
        return np.concatenate(parts)

    def get_time_values(self, strt, end, step=1):
        """
        :return: Evenly spaced array of incremental time stamps, created in microseconds
        """
        counts = np.linspace(strt, end, num=self._count_indexing_num(strt, end, step))
        return pd.to_datetime(pd.Series(self._counts_to_us(counts)), unit='us')

    @staticmethod
    def _count_indexing_num(strt, end, step):
        """Counts the number of elements as result of numpy array indexing """
        num = (end - strt) // step
        # if (end - strt) % step != 0:
        #     num += 1
        return num + 1

    def _counts_to_us(self, counts):
        # Converted to time in microseconds as integer, drastically raises efficiency while maintaining precision
        if floor(self.FAC_TO_US) == self.FAC_TO_US:  # Is an int
            return counts * int(self.FAC_TO_US)
        else:
            return (counts * self.FAC_TO_US).astype(np.int64)

    def get_time_values_delta(self, strt, end, step):
        """ For external export """
        counts = np.linspace(strt, end, num=self._count_indexing_num(strt, end, step))
        return pd.to_timedelta(pd.Series(self._counts_to_us(counts)), unit='us')

    def time_str_to_count(self, time):
        """
        Within range [0, maximum sample count)
        """
        return self.keep_range(self.pd_time_to_count(pd.Timestamp(time) - self.EPOCH_START))

    def pd_time_to_count(self, t):
        """
        :param t: pandas time object
        .. note:: Doesn't check valid record indexing
        """
        # Optimization: integer instead of float arithmetic while preserving accuracy
        t_us = t // self.UNIT_1US
        return t_us * self.spl_rate // (10 ** 6)

    def keep_range(self, count):
        """ Make sure the index into record stays within bounds """
        return min(max(0, count), self.COUNT_END)

    def _in_range(self, count):
        return 0 <= count < self.COUNT_END

    def get_shifted_range(self, mid, mid_range):
        strt_in_range = self._in_range(mid - mid_range)
        end_in_range = self._in_range(mid + mid_range)
        if strt_in_range and end_in_range:
            return mid - mid_range, mid + mid_range
        elif strt_in_range:  # End is too large
            return self.COUNT_END - 2 * mid_range, self.COUNT_END
        else:  # Must've been start < 0:
            return 0, 2 * mid_range

    def _count_to_us(self, count):
        """ As an int """
        return count * (10 ** 6) // self.spl_rate

    def count_to_pd_time(self, count):
        return pd.to_datetime(self._count_to_us(count), unit='us')

    def count_to_str(self, count):
        """
        :return: Human-readable time string, specialized to typical ecg range """
        return self.count_to_pd_time(count).strftime(self.FMT_TMLB)[1:-5]

    def ms_to_count(self, t_ms):
        # ic(t_ms, t_ms * self.spl_rate // 1000)
        return t_ms * self.spl_rate // 1000  # Supposedly the value should be whole integer

    def pd_time_to_str(self, t):
        return t.strftime(self.FMT_TMLB)[1:-5]

    def get_range(self):
        """ Human-readable, inclusive start and exclusive end time of current record """
        return [
            self.count_to_pd_time(0),
            self.count_to_pd_time(self._sample_counts_acc[-1])
        ]

    def get_annotation_indices(self, strt, end):
        """ Find the inclusive start and exclusive end indices for annotations in the range of sample count

        Returns -1, -1 if no annotations within range """
        strt_ms = self._count_to_us(strt) // 1000
        end_ms = self._count_to_us(end) // 1000
        if end_ms < self._ann_tm[0] or strt_ms > self._ann_tm[-1]:
            return -1, -1  # No indices in range
        else:
            idx_strt = bisect_left(self._ann_tm, strt_ms)
            idx_end = bisect_left(self._ann_tm, end_ms)
            # ic(strt_ms, end_ms, idx_strt, idx_end)
            if idx_strt == self._num_ann:
                idx_strt -= 1
            if idx_end == self._num_ann:
                idx_end -= 1
            return idx_strt, idx_end

    def DEBUG_get_rand_range(self, N=10000):
        """ Returns random range of length `N`, for debugging purposes only """
        strt = randint(0, self.COUNT_END - N)
        return strt, strt + N

    @staticmethod
    def example(path=DATA_PATH.joinpath(record_nm)):
        """Fast process a simple example for testing """
        return EcgRecord(path)


if __name__ == "__main__":
    help(EcgRecord)
